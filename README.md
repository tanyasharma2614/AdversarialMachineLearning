## **Description**:

This project is focused on exploring the vulnerabilities of Convolutional Neural Networks (CNNs) through the lens of adversarial machine learning. By analyzing CNN models trained on the MNIST and CIFAR-10 datasets, we delve into understanding their susceptibility to adversarial attacks. The project employs Python, PyTorch, NumPy, and Matplotlib libraries for implementation and visualization.

## **Features**:

• Dataset Analysis: Conducted thorough analysis on MNIST and CIFAR-10 datasets.

• Baseline Accuracy: Achieved impressive baseline accuracies of 98.17% for MNIST and 61.35% for CIFAR-10 using CNN architectures.

• Adversarial Attacks: Implemented adversarial attacks such as Fast Gradient Sign Method (FGSM) and iterative FGSM (iFGSM).

• Accuracy Reduction: Demonstrated the efficacy of adversarial attacks by achieving an average accuracy reduction to 13.92% for MNIST and 10.35% for CIFAR-10.

## **Implementation**:

• Utilized Python for programming logic.

• Leveraged PyTorch for building and training CNN models.

• Employed NumPy for numerical computations.

• Utilized Matplotlib for data visualization and result analysis.

## **How to Use:**

1. Clone the repository to your local machine.

2. Ensure you have Python, PyTorch, NumPy, and Matplotlib installed.

3. Navigate to the project directory.

4. Run the provided scripts or notebooks to reproduce the experiments and results.

5. Experiment with different parameters, architectures, or datasets to further explore adversarial machine learning.
